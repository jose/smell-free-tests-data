### Test Smells:

<br />

**1 – Abnormal UTF-Use (AUU) [1, 17]:**

*Description:* Test suites change the default behavior of the unit-testing framework.

*Impact:* Harder to understand and maintain.

<br />

**2 – Anonymous Test (AT) [1, 17]:**

*Description:* Test cases with non-descriptive names.

*Impact:* Harder to understand and maintain.

*Alternative Designations:* Test-Method Category Name [17].

<br />

**3 – Assertion Roulette (AR) [8, 13, 16, 19, 20]:**

*Description:* A test case has several unexplained assertions. This test smell can arise for one of two reasons:

 1. A test case has an excessive number of assertions (typically because the test case is inspecting too much functionality);
  
 2. A test case has multiple assertions without assertion messages.
  
*Impact:* When the test fails, it is difficult to identify the exact assertion that failed (hinders comprehensibility and maintainability).

<br />

**4 – Brittle Assertion (BA)  [1, 11]:**

*Description:* One or more assertions in a test case check values that said test case does not manipulate (the test is checking too much).

*Impact:* Less effective (the test may fail when it should not) and harder to maintain.

<br />

**5 – Conditional Test Logic (CTL) [1, 13, 15, 16, 19]:**

*Description:* Test cases have control structures that may prevent the execution of specific statements (i.e., the test cases have many execution paths).

*Impact:* Behave unpredictably and are less effective (this smell may hamper test coverage and fault detection effectiveness). Such tests are also harder to understand and maintain.

*Alternative Designations:* Indented Test [6, 13] and Guarded Test [17].

*Variants:* Control Logic (ConL) [1, 17] - test cases use methods such as debug or halt to control the execution flow.

<br />

**6 – Constructor Initialization (CI) [1, 15, 16, 21]:**

*Description:* A test suite uses a constructor to initialize the fields; instead, tests should have set up methods (using constructors is a bad practice and, as such, should be avoided).

*Impact:* Harder to understand.

<br />

**7 – Dead Field (DF) [1, 9]:**

*Description:* Fields are initialized in an implicit setup but not used by the test cases.

*Impact:* This smell affects understandability and leads to slower tests (unnecessary work).

*Alternative Designations:* Unused Shared-Fixture Variables [17].

<br />

**8 – Default Test (DT) [1, 15, 16]:**

*Description:* Using the example test suites automatically generated by Android Studio.

*Impact:* If not removed, developers may add test cases to such test suites.

<br />

**9 – Duplicate Assert (DA) [1, 15, 16, 21]:**

*Description:* A test case contains several assertions that check the same condition (i.e., a test case contains two or more assertions with the same parameters). This smell arises even if the test case checks the same condition with different values.

*Impact:* Harder to understand and maintain.

<br />

**10 – Eager Test (ET) [13, 14, 16, 19, 20]:**

*Description:* A test case checks multiple methods of the class under test (i.e., it verifies too much functionality). Typically, this smell is said to occur when a test case checks two or more methods of the class under test.

*Impact:* Harder to understand and maintain.

<br />

**11 – Empty Shared-Fixture (ESF) [1, 17]:**

*Description:* A test suite contains an implicit setup with an empty body.

*Impact:* Harder to understand and maintain.

<br />

**12 – Empty Test (EmT) [1, 15, 16, 19, 21]:**

*Description:* Test cases without executable statements.

*Impact:* Less effective (empty tests always pass, thus giving a false sense of security).

*Variants:* Comments Only Test (COT) [17] - corresponds to a commented-out test case.

<br />

**13 – Erratic Test (ErT) [13]:**

*Description:* Tests exhibit erratic behavior - they might give different results in different runs (e.g., different people run the same tests but obtain different results).

*Impact:* Less effective.

<br />

**14 – Exception Handling (EH) [1, 15, 16]:**

*Description:* Tests use throw/catch statements instead of JUnit's exception handling.

*Impact:* Harder to understand and maintain. his smell also makes tests less effective.

<br />

**15 – For Testers Only (FTO) [4, 5, 8, 13, 20]:**

*Description:* The production class contains code exclusively used by tests.

*Impact:* This test smell negatively impacts the understandability and maintainability of production code since it makes the system under test more complex.

<br />

**16 – Fragile Test (FT) [13]:**

*Description:* Tests start to fail to compile/run due to unrelated changes to the system under test. Fragile tests can also start to fail in situations where nothing was changed.

*Impact:* This smell leads to increased maintenance costs.

<br />

**17 – Frequent Debugging (FD) [13]:**

*Description:* It is often necessary to manually debug the tests to determine the cause of failures. This test smell might arise due to (1) the lack of available information or (2) infrequently run tests.

*Impact:* Such tests are less effective and harder to understand and maintain.

*Alternative Designations:* Manual Debugging [13].

<br />

**18 – General Fixture (GF) [4, 9, 13, 19, 20]:**

*Description:* An implicit setup is excessively general/large, so the test cases do not access the entirety of the fixture.

*Impact:* This smell affects comprehensibility and causes tests to run slower due to the unnecessary extra work. It can also make the tests fragile (i.e., affects maintainability).

<br />

**19 – Hard-to-Test Code (HTTC) [13]:**

*Description:* The system under test has properties that make it inherently difficult to test (e.g., highly coupled code). It is also possible to have test code that is difficult to test.

*Impact:* Less effective automatically generated tests. Harder to manually write tests.

<br />

**20 – Ignored Test (IgT) [1, 15, 16, 19, 21]:**

*Description:* Test case/suite uses the _@Ignore_ annotation, thus impeding it from running.

*Impact:* Test suites become harder to understand. Causes compilation time overhead.

<br />

**21 – Indirect Testing (IT) [4, 8, 13, 14, 20]:**

*Description:* A test case performs tests on classes other than the one under test. This smell may arise because the test case is (indirectly) checking the respective production class using methods of other classes.

*Impact:* This smell negatively affects the comprehensibility and maintainability of test cases. It can also hamper the debugging process.

<br />

**22 – Lack of Cohesion of Methods (LCM) [1, 9]:**

*Description:* Unrelated test cases are arranged into a test suite (i.e., they are not cohesive).

*Impact:* Harder to understand and maintain.

<br />

**23 – Lazy Test (LT) [1, 4, 5, 15, 20]:**

*Description:* Multiple test cases in a test suite check the same production method.

*Impact:* This smell can hinder maintainability.

<br />

**24 – Likely Ineffective Object-Comparison (LIOC) [1, 17]:**

*Description:* A test case has one or more object comparisons that will never fail (e.g., a test case compares an object with itself).

*Impact:* Less effective.

<br />

**25 – Magic Number Test (MNT) [1, 15, 16, 19, 21]:**

*Description:* A test case uses unexplained/undocumented numerical values.

*Impact:* This test smell hampers the understandability and maintainability of test cases (it is harder to understand the meaning and purpose of such values).

<br />

**26 – Manual Intervention (MI) [13]:**

*Description:* Tests that require some form of manual action to run.

*Impact:* Such tests are likely to be run less frequently due to the effort they require. As such, this smell makes tests less effective.

<br />

**27 – Mixed Selectors (MS) [1, 17]:**

*Description:* A class contains both production methods and test cases.

*Impact:* This test smell negatively impacts understandability and maintainability.

*Detection Strategy:* A class contains both test and production code.

<br />

**28 – Mystery Guest (MG) [4, 8, 13, 20, 21]:**

*Description:* Tests use external resources (such as files or databases); hence, they are not self-contained.

*Impact:* Harder to understand and maintain due to the lack of available information. The usage of external resources also introduces hidden dependencies.

<br />

**29 – Non-Java Smells (NJS):**

*Description:* This does not correspond to a specific smell. Instead, it represents a set of simple test smells associated with concepts unrelated to Java. We have decided to combine the following smells into this category:

 1. *Empty Method Category [17]:* Test case with an empty method category.
  
 2. *Empty Test-Method Category [17]:* Test case with an empty test method category.
  
 3. *TTCN-3 Smells [3]:* Set of test smells specific to TTCN-3 test suites.
  
 4. *Unclassified Method Category [17]:* Test cases not organized by a method-category.

<br />

**30 – Obscure In-line Setup (OISS) [1, 9]:**

*Description:* A test case contains an excessive amount of setup functionality (an in-line setup should only have what is required to understand the test).

*Impact:* Harder to understand and maintain.

*Variants:* Max Instance Variables (MIV) [17] - Overly large fixture.

*Note:* The acceptable amount of setup information in a test case is dependent on the characteristics of the respective test and the production class.

<br />

**31 – Overcommented Test (OCT) [1, 17]:**

*Description:* A test contains too many comments.

*Impact:* This smell can make the tests harder to understand (which is the opposite of what comments should do).

<br />

**32 – Overreferencing (OF) [1, 17]:**

*Description:* A test case that references classes an excessive number of times.

*Impact:* This test smell makes the tests more difficult to understand and maintain.

*Note:* The acceptable amount of referenced classes in a test case is dependent on the characteristics of the respective test and the production class.

<br />

**33 – Proper Organization (PO) [1, 17]:**

*Description:* Poorly organized test cases that do not respect testing conventions.

*Impact:* Harder to understand and maintain.

<br />

**34 – Redundant Assertion (RA) [1, 15, 16, 21]:**

*Description:* Test cases have assertions that are permanently true/false (e.g., assertions with equal values for the actual and expected parameters).

*Impact:* This smell makes the tests less effective (it can give a false sense of security).

*Detection:* A test case has at least one assertion with the same values for the actual and expected parameters.

<br />

**35 – Redundant Print (RP) [1, 15, 16]:**

*Description:* Test cases have (unnecessary) print statements.

*Impact:* May hinder test effectiveness (print statements consume both time and resources).

*Variants:* Transcripting Test (TT) [17] - corresponds to printing/logging to the console.

<br />

**36 – Resource Optimism (RO) [8, 16, 18, 19, 20]:**

*Description:* Test cases that make optimistic assumptions about the existence/state of external resources.

*Impact:* This smell can lead to non-deterministic test results.

<br />

**37 – Returning Assertion (ReA) [1]:**

*Description:* A test case contains assertions and also returns a value.

*Impact:* This smell affects maintainability and comprehensibility.

<br />

**38 – Rotten Green Tests (RGT) [1, 2, 7]:**

*Description:* Test cases affected by this smell can pass without executing at least one assertion, thus giving a false sense of security.

*Impact:* Less effective.

*Variants:* Early Returning Test (ERT) - the test case does not execute certain assertions because it returns a value too early.

<br />

**39 – Sensitive Equality (SE) [8, 14, 16, 18, 20]:**

*Description:* A test has assertions that perform equality checks using the _toString_ method.

*Impact:* Test cases become dependent on (irrelevant) details of the String used in the comparison. Moreover, if the _toString_ method for an object changes, the test starts failing.

<br />

**40 – Sleepy Test (ST) [1, 15, 16]:**

*Description:* Temporarily stopping the execution of a test case.

*Impact:* Less effective - pausing a thread can trigger unexpected results.

<br />

**41 – Slow Tests (SloT) [13]:**

*Description:* Tests take a long time to run. This smell can arise as a result of (1) poorly designed test code or (2) the characteristics of the system under test.

*Impact:* Slow tests are less effective and are likely to be run less frequently.

<br />

**42 – Teardown Only Test (TOT) [1, 17]:**

*Description:* Test suites that only specify teardown.

*Impact:* This smell affects maintainability.

<br />

**43 – Test Code Duplication (TCD)  [1, 4, 5, 13, 20]:**

*Description:* Unwanted duplication in the test code. Test code duplication can be present amongst several tests or within the same test.

*Impact:* This smell affects maintainability and comprehensibility.

*Alternative Designations:* Duplicated Code [6].

<br />

**44 – Test Logic in Production (TLP) [13]:**

*Description:* Production code contains logic that should solely be exercised when testing.

*Impact:* This smell makes the system under test more complex and fault-prone (serious problems can arise when running the test-specific code in a production environment).

<br />

**45 – Test Maverick (TM) [1, 9]:**

*Description:* A test suite contains test cases independent of the existing implicit setup.

*Impact:* Run slower (unnecessary extra work). Harder to comprehend and maintain.

<br />

**46 – Test Pollution (TP) [1, 10]:**

*Description:* Dependent tests that can use (i.e., read/write) shared resources.

*Impact:* This test smell negatively impacts understandability and maintainability.

<br />

**47 – Test Redundancy (TR) [1, 12]:**

*Description:* One or more test cases can be removed without affecting the fault detection effectiveness of the test suite.

*Impact:* Redundant test cases only make the test suite harder to understand and maintain.

<br />

**48 – Test Run War (TRW) [1, 4, 5, 13, 20]:**

*Description:* Tests allocate resources (e.g., temporary files) used by various people.

*Impact:* These tests may fail if different people run them simultaneously.

<br />

**49 – Test-Class Name (TCN) [1, 17]:**

*Description:* Test suites with non-descriptive names.

*Impact:* Harder to understand and maintain.

<br />

**50 – Unknown Test (UT) [1, 14, 15, 16, 21]:**

*Description:* Test cases without valid assertions or _@Test(expected)_ annotations.

*Impact:* Less effective because there are no assertions to check whether the results are as expected. Harder to understand and maintain as there are no assertions to examine, thereby making it harder to deduce the purpose of the test cases (this is especially apparent when test cases have non-descriptive names).

*Alternative Designations:* Assertionless [6] and Assertionless Test [17].

*Variants:* Under-the-carpet Assertion (UCA) [17] - corresponds to commenting-out assertions in a test case (if only failing assertions are commented-out from the test case, then it is considered an Under-the-carpet failing Assertion (UCFA) [17] smell).

<br />

**51 – Unused Inputs (UI) [1, 11]:**

*Description:* A test case has no assertions to check the particular values that said test case manipulates (the test case is checking too little).

*Impact:* Less effective (as it may be unable to reveal faults) and harder to maintain.

<br />

**52 – Unusual Test Order (UTO) [1, 17]:**

*Description:* A test that directly calls other tests.

*Impact:* Such tests may manifest erratic behavior (i.e., they are less effective).

<br />

**53 – Vague Header Setup (VHS) [1, 9]:**

*Description:* Fields are initialized in the class header rather than in an implicit setup.

*Impact:* Harder to understand and maintain.

<br />

**54 – Verbose Test (VT) [6, 13, 17, 19, 21]:**

*Description:* A test case contains an excessive number of statements (i.e., the test is unnecessarily long). As a result, the test code is neither clean nor simple.

*Impact:* The excessive number of statements makes the tests harder to understand and maintain. Such tests are also more likely to contain other types of test smells [8].

*Alternative Designations:* Complex Test [13], Long Test [17] and Obscure Test [13].

*Note:* The acceptable number of statements in a test case is dependent on the situation at hand (e.g., 20 statements may be too much in some situations and not enough in others).

<br />

---

### Bibliography:

[1] Wajdi Aljedaani, Anthony Peruma, Ahmed Aljohani, Mazen Alotaibi, Mohamed Wiem Mkaouer, Ali Ouni, Christian D Newman, Abdullatif Ghallab, and Stephanie Ludi. Test smell detection tools: A systematic mapping study. Evaluation and Assessment in Software Engineering, pages 170–180, 2021.

[2] Vincent Aranega, Julien Delplanque, Matias Martinez, Andrew P Black, Stéphane Ducasse, Anne Etien, Christopher Fuhrman, and Guillermo Polito. Rotten green tests in java, pharo and python. Empirical Software Engineering, 26(6):1–41, 2021.

[3] Paul Baker, Dominic Evans, Jens Grabowski, Helmut Neukirchen, and Benjamin Zeiss. Trex-the refactoring and metrics tool for ttcn-3 test specifications. In Testing: Academic & Industrial Conference-Practice And Research Techniques (TAIC PART’06), pages 90–94. IEEE, 2006.

[4] Gabriele Bavota, Abdallah Qusef, Rocco Oliveto, Andrea De Lucia, and Dave Binkley. Are test smells really harmful? an empirical study. Empirical Software Engineering, 20(4):1052–1094, 2015.

[5] Gabriele Bavota, Abdallah Qusef, Rocco Oliveto, Andrea De Lucia, and David Binkley. An empirical analysis of the distribution of unit test smells and their impact on software maintenance. In 2012 28th IEEE International Conference on Software Maintenance (ICSM), pages 56–65. IEEE, 2012.

[6] Manuel Breugelmans and Bart Van Rompaey. Testq: Exploring structural and maintenance characteristics of unit test suites. In WASDeTT-1: 1st International Workshop on Advanced Software Development Tools and Techniques. Citeseer, 2008.

[7] Julien Delplanque, Stéphane Ducasse, Guillermo Polito, Andrew P Black, and Anne Etien. Rotten green tests. In 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), pages 500–511. IEEE, 2019.

[8]  Giovanni Grano, Fabio Palomba, Dario Di Nucci, Andrea De Lucia, and Harald C Gall. Scented since the beginning: On the diffuseness of test smells in automatically generated test code. Journal of Systems and Software, 156:312–327, 2019.

[9] Michaela Greiler, Arie Van Deursen, and Margaret-Anne Storey. Automated detection of test fixture strategies and smells. In 2013 IEEE Sixth International Conference on Software Testing, Verification and Validation, pages 322–331. IEEE, 2013.

[10] Alex Gyori, August Shi, Farah Hariri, and Darko Marinov. Reliable testing: Detecting state-polluting tests to prevent test dependency. In Proceedings of the 2015 International Symposium on Software Testing and Analysis, pages 223–233, 2015.

[11] Chen Huo and James Clause. Improving oracle quality by detecting brittle assertions and unused inputs in tests. In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering, pages 621–631, 2014.

[12] Negar Koochakzadeh and Vahid Garousi. Tecrevis: a tool for test coverage and test redundancy visualization. In International Academic and Industrial Conference on Practice and Research Techniques, pages 129–136. Springer, 2010.

[13] Gerard Meszaros. xUnit test patterns: Refactoring test code. Pearson Education, 2007.

[14] Annibale Panichella, Sebastiano Panichella, Gordon Fraser, Anand Ashok Sawant, and Vincent J. Hellendoorn. Revisiting test smells in automatically generated tests: Limitations, pitfalls, and opportunities. In 2020 IEEE International Conference on Software Maintenance and Evolution (ICSME), pages 523–533, 2020.

[15] Anthony Peruma, Khalid Saeed Almalki, Christian D Newman, Mohamed Wiem Mkaouer, Ali Ouni, and Fabio Palomba. On the distribution of test smells in open source android applications: An exploratory study. 2019.

[16] Anthony Peruma, Mohamed Wiem Mkaouer, Khalid Almalki, Christian D. Newman, Ali Ouni, and Fabio Palomba. Software unit test smells. https://testsmells.org/. Accessed: 2021-12-25.

[17] Stefan Reichhart, Tudor Gîrba, and Stephane Ducasse. Rule-based assessment of test quality. J. Object Technol., 6(9):231–251, 2007.

[18] Davide Spadini, Fabio Palomba, Andy Zaidman, Magiel Bruntink, and Alberto Bacchelli. On the relation of test smells to software code quality. In 2018 IEEE International Conference on Software Maintenance and Evolution (ICSME), pages 1–12. IEEE, 2018.

[19] Davide Spadini, Martin Schvarcbacher, Ana-Maria Oprescu, Magiel Bruntink, and Alberto Bacchelli. Investigating severity thresholds for test smells. In Proceedings of the 17th International Conference on Mining Software Repositories, pages 311–321, 2020.

[20] Arie Van Deursen, Leon Moonen, Alex Van Den Bergh, and Gerard Kok. Refactoring test code. In Proceedings of the 2nd international conference on extreme programming and flexible processes in software engineering (XP2001), pages 92–95. Citeseer, 2001.

[21] Tássio Virgínio, Railana Santana, Luana Almeida Martins, Larissa Rocha Soares, Heitor Costa, and Ivan Machado. On the influence of test smells on test coverage. In Proceedings of the XXXIII Brazilian Symposium on Software Engineering, pages 467–471, 2019.
